{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _init_paths\n",
    "from loaders.dataloader import DataLoader\n",
    "from opt import parse_opt\n",
    "from pprint import pprint\n",
    "import argparse\n",
    "import os\n",
    "import os.path as osp\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import h5py\n",
    "from layers.match import AdaptiveReconstruct\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'att_res_weight': 1.0,\n",
      " 'att_weight': 1.0,\n",
      " 'batch_size': 5,\n",
      " 'bidirectional': 1,\n",
      " 'checkpoint_path': 'output',\n",
      " 'dataset': 'refcoco',\n",
      " 'decode_bidirectional': 0,\n",
      " 'exp_id': '',\n",
      " 'gpuid': 0,\n",
      " 'grad_clip': 0.1,\n",
      " 'id': 'mrcn_cmr_with_st',\n",
      " 'imdb_name': 'coco_minus_refer',\n",
      " 'iters': 1250000,\n",
      " 'jemb_dim': 512,\n",
      " 'jemb_drop_out': 0.1,\n",
      " 'lang_rank_weight': 1.0,\n",
      " 'lang_res_weight': 1.0,\n",
      " 'language_eval': 0,\n",
      " 'learning_rate': 0.0004,\n",
      " 'learning_rate_decay_every': 8000,\n",
      " 'learning_rate_decay_start': 8000,\n",
      " 'load_best_score': 1,\n",
      " 'loss_combined': 5.0,\n",
      " 'loss_divided': 1.0,\n",
      " 'losses_log_every': 25,\n",
      " 'margin': 0.1,\n",
      " 'max_iters': 30000,\n",
      " 'net_name': 'res101',\n",
      " 'num_cxt': 5,\n",
      " 'num_sents': -1,\n",
      " 'optim_alpha': 0.8,\n",
      " 'optim_beta': 0.999,\n",
      " 'optim_epsilon': 1e-08,\n",
      " 'rnn_drop_out': 0.2,\n",
      " 'rnn_hidden_size': 512,\n",
      " 'rnn_num_layers': 1,\n",
      " 'rnn_type': 'lstm',\n",
      " 'sample_ratio': 0.3,\n",
      " 'save_checkpoint_every': 2000,\n",
      " 'seed': 24,\n",
      " 'seq_per_ref': 3,\n",
      " 'splitBy': 'unc',\n",
      " 'start_from': None,\n",
      " 'tag': 'notime',\n",
      " 'use_IoU': 1,\n",
      " 'variable_lengths': 1,\n",
      " 'vis_feats_type': 'res101',\n",
      " 'vis_res_weight': 0.01,\n",
      " 'visual_cxt_type': 'frcn',\n",
      " 'visual_drop_out': 0.2,\n",
      " 'visual_fuse_mode': 'concat',\n",
      " 'visual_init_norm': 20,\n",
      " 'visual_rank_weight': 1.0,\n",
      " 'visual_sample_ratio': 0.3,\n",
      " 'visual_use_bn': -1,\n",
      " 'visual_use_cxt': 1,\n",
      " 'weight_decay': 0.0005,\n",
      " 'window_scale': 2.5,\n",
      " 'with_st': 1,\n",
      " 'word_drop_out': 0.5,\n",
      " 'word_embedding_size': 512,\n",
      " 'word_vec_size': 512}\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "# Data input settings\n",
    "parser.add_argument('--dataset', type=str, default='refcoco', help='name of dataset')\n",
    "parser.add_argument('--splitBy', type=str, default='unc', help='who splits this dataset')\n",
    "parser.add_argument('--start_from', type=str, default=None, help='continuing training from saved model')\n",
    "# FRCN setting\n",
    "parser.add_argument('--imdb_name', default='coco_minus_refer', help='image databased trained on.')\n",
    "parser.add_argument('--net_name', default='res101', help='net_name: res101 or vgg16')\n",
    "parser.add_argument('--iters', default=1250000, type=int, help='iterations we trained for faster R-CNN')\n",
    "parser.add_argument('--tag', default='notime', help='on default tf, don\\'t change this!')\n",
    "parser.add_argument('--vis_feats_type', type=str, default='res101', help='visual features type: vgg16 or res101')\n",
    "# Visual Encoder Setting\n",
    "parser.add_argument('--visual_sample_ratio', type=float, default=0.3, help='ratio of same-type objects over different-type objects')\n",
    "parser.add_argument('--visual_fuse_mode', type=str, default='concat', help='concat or mul')\n",
    "parser.add_argument('--visual_init_norm', type=float, default=20, help='norm of each visual representation')\n",
    "parser.add_argument('--visual_use_bn', type=int, default=-1, help='>0: use bn, -1: do not use bn in visual layer')    \n",
    "parser.add_argument('--visual_use_cxt', type=int, default=1, help='if we use contxt')\n",
    "parser.add_argument('--visual_cxt_type', type=str, default='frcn', help='frcn or res101')\n",
    "parser.add_argument('--visual_drop_out', type=float, default=0.2, help='dropout on visual encoder')\n",
    "parser.add_argument('--window_scale', type=float, default=2.5, help='visual context type')\n",
    "# Visual Feats Setting\n",
    "parser.add_argument('--with_st', type=int, default=1, help='if incorporating same-type objects as contexts')\n",
    "parser.add_argument('--num_cxt', type=int, default=5, help='how many surrounding objects do we use')\n",
    "# Language Encoder Setting\n",
    "parser.add_argument('--word_embedding_size', type=int, default=512, help='the encoding size of each token')\n",
    "parser.add_argument('--word_vec_size', type=int, default=512, help='further non-linear of word embedding')\n",
    "parser.add_argument('--word_drop_out', type=float, default=0.5, help='word drop out after embedding')\n",
    "parser.add_argument('--bidirectional', type=int, default=1, help='bi-rnn')\n",
    "parser.add_argument('--rnn_hidden_size', type=int, default=512, help='hidden size of LSTM')\n",
    "parser.add_argument('--rnn_type', type=str, default='lstm', help='rnn, gru or lstm')\n",
    "parser.add_argument('--rnn_drop_out', type=float, default=0.2, help='dropout between stacked rnn layers')\n",
    "parser.add_argument('--rnn_num_layers', type=int, default=1, help='number of layers in lang_encoder')\n",
    "parser.add_argument('--variable_lengths', type=int, default=1, help='use variable length to encode') \n",
    "# Joint Embedding setting\n",
    "parser.add_argument('--jemb_drop_out', type=float, default=0.1, help='dropout in the joint embedding')\n",
    "parser.add_argument('--jemb_dim', type=int, default=512, help='joint embedding layer dimension')\n",
    "# Reconstruct Settings\n",
    "parser.add_argument('--decode_bidirectional', type=int, default=0, help='whther to use bidirection LSTM in reconstrcution')\n",
    "# Loss Setting\n",
    "parser.add_argument('--att_weight', type=float, default=1.0, help='weight on attribute prediction')\n",
    "parser.add_argument('--visual_rank_weight', type=float, default=1.0, help='weight on paired (ref, sent) over unpaired (neg_ref, sent)')\n",
    "parser.add_argument('--lang_rank_weight', type=float, default=1.0, help='weight on paired (ref, sent) over unpaired (ref, neg_sent)')\n",
    "parser.add_argument('--margin', type=float, default=0.1, help='margin for ranking loss')\n",
    "parser.add_argument('--lang_res_weight', type=float, default=1.0, help='weight on language reconstruction loss')\n",
    "parser.add_argument('--vis_res_weight', type=float, default=0.01, help='weight on visual reconstruction loss')\n",
    "parser.add_argument('--att_res_weight', type=float, default=1.0, help='weight on attribute reconstruction loss')\n",
    "parser.add_argument('--loss_combined', type=float, default=5.0, help='weight on loss_combined')\n",
    "parser.add_argument('--loss_divided', type=float, default=1.0, help='weight on loss_divided' )\n",
    "# Optimization: General\n",
    "parser.add_argument('--max_iters', type=int, default=30000, help='max number of iterations to run')\n",
    "parser.add_argument('--sample_ratio', type=float, default=0.3, help='ratio of same-type objects over different-type objects')\n",
    "parser.add_argument('--batch_size', type=int, default=5, help='batch size in number of images per batch')\n",
    "parser.add_argument('--grad_clip', type=float, default=0.1, help='clip gradients at this value')\n",
    "parser.add_argument('--seq_per_ref', type=int, default=3, help='number of expressions per object during training')\n",
    "parser.add_argument('--learning_rate_decay_start', type=int, default=8000, help='at what iter to start decaying learning rate')\n",
    "parser.add_argument('--learning_rate_decay_every', type=int, default=8000, help='every how many iters thereafter to drop LR by half')\n",
    "parser.add_argument('--optim_epsilon', type=float, default=1e-8, help='epsilon that goes into denominator for smoothing')\n",
    "parser.add_argument('--learning_rate', type=float, default=4e-4, help='learning rate')\n",
    "parser.add_argument('--optim_alpha', type=float, default=0.8, help='alpha for adam')\n",
    "parser.add_argument('--optim_beta', type=float, default=0.999, help='beta used for adam')\n",
    "parser.add_argument('--weight_decay', type=float, default=0.0005, help='weight decay for adam')\n",
    "# Evaluation/Checkpointing\n",
    "parser.add_argument('--num_sents', type=int, default=-1, help='how many images to use when periodically evaluating the validation loss? (-1 = all)')\n",
    "parser.add_argument('--save_checkpoint_every', type=int, default=2000, help='how often to save a model checkpoint?')\n",
    "parser.add_argument('--checkpoint_path', type=str, default='output', help='directory to save models')   \n",
    "parser.add_argument('--language_eval', type=int, default=0, help='Evaluate language as well (1 = yes, 0 = no)?')\n",
    "parser.add_argument('--losses_log_every', type=int, default=25, help='How often do we snapshot losses, for inclusion in the progress dump? (0 = disable)')\n",
    "parser.add_argument('--load_best_score', type=int, default=1, help='Do we load previous best score when resuming training.')      \n",
    "parser.add_argument('--use_IoU', type=int, default=1, help='Whether to use IoU evaluation or not')\n",
    "# misc\n",
    "parser.add_argument('--id', type=str, default='mrcn_cmr_with_st', help='an id identifying this run/job.')\n",
    "parser.add_argument('--seed', type=int, default=24, help='random number generator seed to use')\n",
    "parser.add_argument('--gpuid', type=int, default=0, help='which gpu to use, -1 = use CPU')\n",
    "parser.add_argument('--exp_id', type=str, default='', help='experiment id')\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "opt = vars(args)\n",
    "pprint(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loader loading data.json:  ../cache/prepro/refcoco_unc/data.json\n",
      "vocab size is  1999\n",
      "object cateogry size is  80\n",
      "we have 19994 images.\n",
      "we have 196771 anns.\n",
      "we have 50000 refs.\n",
      "we have 142210 sentences.\n",
      "label_length is  10\n",
      "Loader loading data.h5:  ../cache/prepro/refcoco_unc/data.h5\n",
      "assigned 16994 images to split train\n",
      "assigned 750 images to split testB\n",
      "assigned 1500 images to split val\n",
      "assigned 750 images to split testA\n"
     ]
    }
   ],
   "source": [
    "opt['dataset_splitBy'] = opt['dataset'] + '_' + opt['splitBy']\n",
    "data_json = osp.join('../cache/prepro', opt['dataset_splitBy'], 'data.json')\n",
    "data_h5 = osp.join('../cache/prepro', opt['dataset_splitBy'], 'data.h5')\n",
    "loader = DataLoader(data_h5=data_h5, data_json=data_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using config:\n",
      "{'ANCHOR_RATIOS': [0.5, 1, 2],\n",
      " 'ANCHOR_SCALES': [4, 8, 16, 32],\n",
      " 'DATA_DIR': '/datasets/Kranthi/Maitreya/Manish/ARN/pyutils/mask-faster-rcnn/data',\n",
      " 'DATA_DIR_IMAGE': '/datasets/Kranthi/Maitreya/Manish/ARN/data/images',\n",
      " 'EXP_DIR': 'res101',\n",
      " 'MASK_SIZE': 14,\n",
      " 'MATLAB': 'matlab',\n",
      " 'MOBILENET': {'DEPTH_MULTIPLIER': 1.0,\n",
      "               'FIXED_LAYERS': 5,\n",
      "               'REGU_DEPTH': False,\n",
      "               'WEIGHT_DECAY': 4e-05},\n",
      " 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),\n",
      " 'POOLING_ALIGN': False,\n",
      " 'POOLING_MODE': 'crop',\n",
      " 'POOLING_SIZE': 7,\n",
      " 'RESNET': {'FIXED_BLOCKS': 1, 'MAX_POOL': False},\n",
      " 'RNG_SEED': 3,\n",
      " 'ROOT_DIR': '/datasets/Kranthi/Maitreya/Manish/ARN/pyutils/mask-faster-rcnn',\n",
      " 'TEST': {'BBOX_REG': True,\n",
      "          'HAS_RPN': True,\n",
      "          'MAX_SIZE': 1000,\n",
      "          'MODE': 'nms',\n",
      "          'NMS': 0.3,\n",
      "          'PROPOSAL_METHOD': 'gt',\n",
      "          'RPN_NMS_THRESH': 0.7,\n",
      "          'RPN_POST_NMS_TOP_N': 300,\n",
      "          'RPN_PRE_NMS_TOP_N': 6000,\n",
      "          'RPN_TOP_N': 5000,\n",
      "          'SCALES': [600],\n",
      "          'SVM': False},\n",
      " 'TRAIN': {'ASPECT_GROUPING': False,\n",
      "           'BATCH_SIZE': 256,\n",
      "           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],\n",
      "           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],\n",
      "           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],\n",
      "           'BBOX_NORMALIZE_TARGETS': True,\n",
      "           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': True,\n",
      "           'BBOX_REG': True,\n",
      "           'BBOX_THRESH': 0.5,\n",
      "           'BG_THRESH_HI': 0.5,\n",
      "           'BG_THRESH_LO': 0.0,\n",
      "           'BIAS_DECAY': False,\n",
      "           'DISPLAY': 20,\n",
      "           'DOUBLE_BIAS': False,\n",
      "           'FG_FRACTION': 0.25,\n",
      "           'FG_THRESH': 0.5,\n",
      "           'FROM_FRCN': False,\n",
      "           'GAMMA': 0.1,\n",
      "           'HAS_RPN': True,\n",
      "           'IMS_PER_BATCH': 1,\n",
      "           'LEARNING_RATE': 0.001,\n",
      "           'MAX_SIZE': 1000,\n",
      "           'MOMENTUM': 0.9,\n",
      "           'PROPOSAL_METHOD': 'gt',\n",
      "           'RPN_BATCHSIZE': 256,\n",
      "           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],\n",
      "           'RPN_CLOBBER_POSITIVES': False,\n",
      "           'RPN_FG_FRACTION': 0.5,\n",
      "           'RPN_NEGATIVE_OVERLAP': 0.3,\n",
      "           'RPN_NMS_THRESH': 0.7,\n",
      "           'RPN_POSITIVE_OVERLAP': 0.7,\n",
      "           'RPN_POSITIVE_WEIGHT': -1.0,\n",
      "           'RPN_POST_NMS_TOP_N': 2000,\n",
      "           'RPN_PRE_NMS_TOP_N': 12000,\n",
      "           'SCALES': [600],\n",
      "           'SNAPSHOT_ITERS': 5000,\n",
      "           'SNAPSHOT_KEPT': 3,\n",
      "           'SNAPSHOT_PREFIX': 'res101_mask_rcnn',\n",
      "           'STEPSIZE': [30000],\n",
      "           'SUMMARY_INTERVAL': 180,\n",
      "           'TRUNCATED': False,\n",
      "           'USE_ALL_GT': True,\n",
      "           'USE_FLIPPED': True,\n",
      "           'USE_GT': False,\n",
      "           'WEIGHT_DECAY': 0.0001},\n",
      " 'USE_GPU_NMS': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/datasets/Kranthi/Maitreya/Manish/ARN/tools/../pyutils/mask-faster-rcnn/lib/model/config.py:365: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  yaml_cfg = edict(yaml.load(f))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained-model loaded from [/datasets/Kranthi/Maitreya/Manish/ARN/tools/../lib/mrcn/../../pyutils/mask-faster-rcnn/output/res101/coco_2014_train_minus_refer_valtest+coco_2014_valminusminival/notime/res101_mask_rcnn_iter_1250000.pth].\n",
      "FeatLoader loading [ann] from ../cache/feats/refcoco_unc/mrcn/res101_coco_minus_refer_notime_ann_feats.h5 [feat_dim 2048]\n"
     ]
    }
   ],
   "source": [
    "feats_dir = '%s_%s_%s' % (args.net_name, args.imdb_name, args.tag)\n",
    "head_feats_dir = osp.join('../cache/feats/', opt['dataset_splitBy'], 'mrcn', feats_dir)\n",
    "\n",
    "loader.prepare_mrcn(head_feats_dir, args)\n",
    "\n",
    "ann_feats = osp.join('../cache/feats', opt['dataset_splitBy'], 'mrcn',\n",
    "                     '%s_%s_%s_ann_feats.h5' % (opt['net_name'], opt['imdb_name'], opt['tag']))\n",
    "loader.loadFeats({'ann': ann_feats})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kranthi/.conda/envs/pytorch_1.5_a/lib/python3.8/site-packages/torch/nn/functional.py:3289: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\"Default grid_sample and affine_grid behavior has changed \"\n",
      "/home/kranthi/.conda/envs/pytorch_1.5_a/lib/python3.8/site-packages/torch/nn/functional.py:3226: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\"Default grid_sample and affine_grid behavior has changed \"\n"
     ]
    }
   ],
   "source": [
    "data = loader.getBatch('train', opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['labels', 'enc_labels', 'dec_labels', 'ref_ids', 'sent_ids', 'gd_ixs', 'gd_boxes', 'cxt_ann_ids', 'Feats', 'att_labels', 'select_ixs', 'bounds'])\n"
     ]
    }
   ],
   "source": [
    "print((data.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[45477, 45478]\n"
     ]
    }
   ],
   "source": [
    "print(data['ref_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['fc7', 'pool5', 'lfeats', 'dif_lfeats', 'cxt_fc7', 'cxt_lfeats'])\n"
     ]
    }
   ],
   "source": [
    "print(data['Feats'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3, 5, 2048])\n"
     ]
    }
   ],
   "source": [
    "print(data['Feats']['cxt_fc7'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_feats = osp.join('../cache/feats', opt['dataset_splitBy'], 'mrcn',\n",
    "                         '%s_%s_%s_ann_feats.h5' % (opt['net_name'], opt['imdb_name'], opt['tag']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_fts = h5py.File(ann_feats, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(196771, 1024)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_fts['pool5'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kranthi/.conda/envs/pytorch_1.5_a/lib/python3.8/site-packages/torch/nn/modules/rnn.py:47: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "/home/kranthi/.conda/envs/pytorch_1.5_a/lib/python3.8/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "opt['vocab_size'] = loader.vocab_size\n",
    "opt['fc7_dim'] = loader.fc7_dim\n",
    "opt['pool5_dim'] = loader.pool5_dim\n",
    "opt['num_atts'] = loader.num_atts\n",
    "model = AdaptiveReconstruct(opt).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                 lr=opt['learning_rate'],\n",
    "                                 betas=(opt['optim_alpha'], opt['optim_beta']),\n",
    "                                 eps=opt['optim_epsilon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data['labels']\n",
    "enc_labels = data['enc_labels']\n",
    "dec_labels = data['dec_labels']\n",
    "Feats = data['Feats']\n",
    "att_labels, select_ixs = data['att_labels'], data['select_ixs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of device type cuda but got device type cpu for argument #1 'self' in call to _th_index_select",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-284fc5cbb276>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0matt_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attribute_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m scores, losses,_,_,_,_,_ = model(Feats['pool5'], Feats['fc7'], Feats['lfeats'], Feats['dif_lfeats'],\n\u001b[0m\u001b[1;32m      3\u001b[0m                                          Feats['cxt_fc7'], Feats['cxt_lfeats'], labels, enc_labels, dec_labels, att_labels, select_ixs, att_weights)\n\u001b[1;32m      4\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch_1.5_a/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/datasets/Kranthi/Maitreya/Manish/ARN/tools/../lib/layers/match.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pool5, fc7, lfeats, dif_lfeats, cxt_fc7, cxt_lfeats, labels, enc_labels, dec_labels, att_labels, select_ixs, att_weights)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpool5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfc7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlfeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdif_lfeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcxt_fc7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcxt_lfeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matt_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselect_ixs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matt_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_fc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch_1.5_a/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/datasets/Kranthi/Maitreya/Manish/ARN/tools/../lib/layers/lang_encoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_labels)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# embed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_labels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (n, seq_len, word_embedding_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (n, seq_len, word_embedding_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (n, seq_len, word_embedding_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch_1.5_a/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch_1.5_a/lib/python3.8/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    113\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[0;32m~/.conda/envs/pytorch_1.5_a/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1722\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1723\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1724\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of device type cuda but got device type cpu for argument #1 'self' in call to _th_index_select"
     ]
    }
   ],
   "source": [
    "att_weights = loader.get_attribute_weights()\n",
    "scores, losses,_,_,_,_,_ = model(Feats['pool5'], Feats['fc7'], Feats['lfeats'], Feats['dif_lfeats'],\n",
    "                                         Feats['cxt_fc7'], Feats['cxt_lfeats'], labels, enc_labels, dec_labels, att_labels, select_ixs, att_weights)\n",
    "losses['loss'].backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn_encoder.embedding.weight\n",
      "rnn_encoder.mlp.0.weight\n",
      "rnn_encoder.mlp.0.bias\n",
      "rnn_encoder.rnn.weight_ih_l0\n",
      "rnn_encoder.rnn.weight_hh_l0\n",
      "rnn_encoder.rnn.bias_ih_l0\n",
      "rnn_encoder.rnn.bias_hh_l0\n",
      "rnn_encoder.rnn.weight_ih_l0_reverse\n",
      "rnn_encoder.rnn.weight_hh_l0_reverse\n",
      "rnn_encoder.rnn.bias_ih_l0_reverse\n",
      "rnn_encoder.rnn.bias_hh_l0_reverse\n",
      "weight_fc.weight\n",
      "weight_fc.bias\n",
      "sub_attn.fc.weight\n",
      "sub_attn.fc.bias\n",
      "loc_attn.fc.weight\n",
      "loc_attn.fc.bias\n",
      "rel_attn.fc.weight\n",
      "rel_attn.fc.bias\n",
      "sub_encoder.pool5_normalizer.weight\n",
      "sub_encoder.fc7_normalizer.weight\n",
      "loc_encoder.lfeats_normalizer.weight\n",
      "loc_encoder.dif_lfeat_normalizer.weight\n",
      "rel_encoder.vis_feat_normalizer.weight\n",
      "rel_encoder.lfeat_normalizer.weight\n",
      "sub_score.feat_fuse.0.weight\n",
      "sub_score.feat_fuse.0.bias\n",
      "sub_score.feat_fuse.2.weight\n",
      "sub_score.feat_fuse.2.bias\n",
      "loc_score.feat_fuse.0.weight\n",
      "loc_score.feat_fuse.0.bias\n",
      "loc_score.feat_fuse.2.weight\n",
      "loc_score.feat_fuse.2.bias\n",
      "rel_score.feat_fuse.0.weight\n",
      "rel_score.feat_fuse.0.bias\n",
      "rel_score.feat_fuse.2.weight\n",
      "rel_score.feat_fuse.2.bias\n",
      "sub_decoder.mlp.0.weight\n",
      "sub_decoder.mlp.0.bias\n",
      "loc_decoder.mlp.0.weight\n",
      "loc_decoder.mlp.0.bias\n",
      "rel_decoder.mlp.0.weight\n",
      "rel_decoder.mlp.0.bias\n",
      "att_res_loss.att_fc.weight\n",
      "att_res_loss.att_fc.bias\n",
      "lang_res_loss.embedding.weight\n",
      "lang_res_loss.mlp.0.weight\n",
      "lang_res_loss.mlp.0.bias\n",
      "lang_res_loss.rnn.weight_ih_l0\n",
      "lang_res_loss.rnn.weight_hh_l0\n",
      "lang_res_loss.rnn.bias_ih_l0\n",
      "lang_res_loss.rnn.bias_hh_l0\n",
      "lang_res_loss.slr_mlp.0.weight\n",
      "lang_res_loss.slr_mlp.0.bias\n",
      "lang_res_loss.fc.weight\n",
      "lang_res_loss.fc.bias\n",
      "rec_loss.embedding.weight\n",
      "rec_loss.mlp.0.weight\n",
      "rec_loss.mlp.0.bias\n",
      "rec_loss.rnn.weight_ih_l0\n",
      "rec_loss.rnn.weight_hh_l0\n",
      "rec_loss.rnn.bias_ih_l0\n",
      "rec_loss.rnn.bias_hh_l0\n",
      "rec_loss.fc.weight\n",
      "rec_loss.fc.bias\n",
      "sub_mlp.0.weight\n",
      "sub_mlp.0.bias\n",
      "loc_mlp.0.weight\n",
      "loc_mlp.0.bias\n",
      "rel_mlp.0.weight\n",
      "rel_mlp.0.bias\n",
      "feat_fuse.0.weight\n",
      "feat_fuse.0.bias\n"
     ]
    }
   ],
   "source": [
    "for param in model.named_parameters():\n",
    "#     if(\"decoder\" in param[0]):\n",
    "        print(param[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
